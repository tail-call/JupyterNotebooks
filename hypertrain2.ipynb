{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Композиционная теория игр и нейросети: эксперимент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Теория**: И.В.Томилов ivan-tomilov3@yandex.ru\n",
    "- **Реализация**: М.А.Зайцева maria@tail-call.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38932.91s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/scales/.pyenv/versions/3.12.3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in /Users/scales/.pyenv/versions/3.12.3/lib/python3.12/site-packages (0.17.2)\n",
      "Requirement already satisfied: numpy<2 in /Users/scales/.pyenv/versions/3.12.3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: filelock in /Users/scales/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/scales/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/scales/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/scales/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/scales/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/scales/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/scales/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/scales/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/scales/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision 'numpy<2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Переопределение поведения обратного распространения ошибки\n",
    "\n",
    "### [Примерная схема того, как переопределить backward][1]\n",
    "\n",
    "Хочется, чтобы было так:\n",
    "\n",
    "```python\n",
    "class LayerWithCustomGrad(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LayerWithCustomGrad, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(200))\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x * self.weights\n",
    "\n",
    "\n",
    "    def backward(self,grad_of_c): # This gets called during loss.backward()\n",
    "        # grad_of_c comes from the gradient of b*23\n",
    "        grad_of_a = some_operation(grad_of_c)\n",
    "\n",
    "        # perform extra computation\n",
    "        # and more computation\n",
    "\n",
    "        self.weights.grad = another_operation(grad_of_a,grad_of_c)\n",
    "        return grad_of_a # and the grad of parameter \"a\" will receive this\n",
    "\n",
    "\n",
    "layer = LayerWithCustomGrad()\n",
    "\n",
    "a = nn.Parameter(torch.randn(200),requires_grad=True)\n",
    "b = layer(a)\n",
    "c = b*23\n",
    "```\n",
    "\n",
    "Но нужно так:\n",
    "\n",
    "```python\n",
    "class F(torch.autograd.Function):\n",
    "    \"\"\"Both forward and backward are static methods.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weights):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input, weights)\n",
    "        return input*weights\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the inputs: here input and weights\n",
    "        \"\"\"\n",
    "        input, weights = ctx.saved_tensors\n",
    "        grad_input = weights.clone()*grad_output\n",
    "        grad_weights = input.clone()*grad_output\n",
    "        return grad_input, grad_weights\n",
    "\n",
    "class LayerWithCustomGrad(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.rand(10))\n",
    "        self.fn = F.apply\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(x, self.weights)\n",
    "\n",
    ">>> layer = LayerWithCustomGrad()\n",
    ">>> x = torch.randn(10, requires_grad=True)\n",
    ">>> y = layer(x)\n",
    "tensor([ 0.2023,  0.7176,  0.3577, -1.3573,  1.5185,  0.0632,  0.1210,  0.1566,\n",
    "         0.0709, -0.4324], grad_fn=<FBackward>)\n",
    "\n",
    ">>> y.mean().backward()\n",
    ">>> x.grad # i.e. grad_input in F.backward\n",
    "tensor([0.0141, 0.0852, 0.0450, 0.0922, 0.0400, 0.0988, 0.0762, 0.0227, 0.0569,\n",
    "        0.0309])\n",
    ">>> layer.weights.grad # i.e. grad_weights in F.backward\n",
    "tensor([-1.4584, -2.1187,  1.5991,  0.9764,  1.8956, -1.0993, -3.7835, -0.4926,\n",
    "         0.9477, -1.2219])\n",
    "```\n",
    "\n",
    "### [Ещё полезная ссылка про переопределение backward с полезным комментарием][2]\n",
    "\n",
    "Полезный комментарий направляет на [Extending PyTorch][6]. В частности,\n",
    "в ней есть секции [Extending `torch.autograd`][7], [How to use][8]:\n",
    "\n",
    "> 1. Subclass `Function` and implement the `forward()`, (optional)\n",
    "> `setup_context()` and `backward()` methods.\n",
    "> 2. Call the proper methods on the `ctx` argument.\n",
    "> 3. Declare whether your function supports [double backward][9].\n",
    "> 4. Validate whether your gradients are correct using gradcheck.\n",
    "\n",
    "Кроме того, в **Step 4** сказано:\n",
    "\n",
    "> It is recommended that you use [`torch.autograd.gradcheck()`][10] to\n",
    "> check whether your backward function correctly computes gradients of\n",
    "> the forward by computing the Jacobian matrix using your backward\n",
    "> function and comparing the value element-wise with the Jacobian\n",
    "> computed numerically using finite-differencing.\n",
    "\n",
    "Из документации [gradcheck][11]:\n",
    "\n",
    "```python\n",
    "torch.autograd.gradcheck.gradcheck(\n",
    "     func,\n",
    "     inputs,\n",
    "     *,\n",
    "     eps=1e-06,\n",
    "     atol=1e-05,\n",
    "     rtol=0.001,\n",
    "     raise_exception=True,\n",
    "     nondet_tol=0.0,\n",
    "     check_undefined_grad=True,\n",
    "     check_grad_dtypes=False,\n",
    "     check_batched_grad=False,\n",
    "     check_batched_forward_grad=False,\n",
    "     check_forward_ad=False,\n",
    "     check_backward_ad=True,\n",
    "     fast_mode=False,\n",
    "     masked=None\n",
    ")\n",
    "```\n",
    "\n",
    "***NB***: что-то про *Wirtinger and Conjugate Wirtinger derivatives*,\n",
    "очень сложно…\n",
    "\n",
    "### [Определение слоя nn.Linear в pytorch][3]\n",
    "\n",
    "Модуль `Linear` применяет аффинную линейную трансформацию ко входным\n",
    "данным:\n",
    "\n",
    "$$\n",
    "y = xA^T + b,\n",
    "$$\n",
    "\n",
    "где:\n",
    "- $x$ — `input`\n",
    "- $y$ – `forward()`\n",
    "- $A$ — `weight`\n",
    "- $b$ — `bias`\n",
    "\n",
    "Параметры:\n",
    "- `in_features` — размер входа\n",
    "- `out_features` — размер выхода\n",
    "- `bias` — если `True`, модель не будет учить смещение\n",
    "\n",
    "Веса и смещения инициализируются равномерным [распределением Кайминга][12].\n",
    "\n",
    "Определение `forward()`:\n",
    "\n",
    "```python\n",
    "def forward(self, input: Tensor) -> Tensor:\n",
    "    return F.linear(input, self.weight, self.bias)\n",
    "```\n",
    "\n",
    "[Документация][15] `backward(ctx, *grad_outputs)`:\n",
    "\n",
    "> It must accept a context `ctx` as the first argument, followed by as\n",
    "> many outputs as the `forward()` returned (None will be passed in for\n",
    "> non tensor outputs of the forward function), and it should return as\n",
    "> many tensors, as there were inputs to `forward()`. Each argument is\n",
    "> the gradient w.r.t the given output, and each returned value should\n",
    "> be the gradient w.r.t. the corresponding input. If an input is not\n",
    "> a Tensor or is a Tensor not requiring grads, you can just pass `None`\n",
    "> as a gradient for that input.\n",
    "\n",
    "### [torch.diag() для диагональной матрицы, на которую надо будет умножать входящий градиент][4]\n",
    "\n",
    "```python\n",
    "torch.diag(\n",
    "     # Вектор или матрица\n",
    "     input,\n",
    "     # 0 – главная диагональ, > 0 – над главной, < 0 – под главной\n",
    "     diagonal=0,\n",
    "     *,\n",
    "     # Тензор, в который выводится результат\n",
    "     out=None\n",
    ") → Tensor\n",
    "```\n",
    "\n",
    "Если `input` вектор — возвращает квадратный двухмерный тензор с\n",
    "элементами `input` в диагонали.\n",
    "\n",
    "Если `input` матрица – возвращает одномерный тензор с диагональными\n",
    "элементами из `input`.\n",
    "\n",
    "### [Диагональ нужно будет генерировать на базе torch.bernoulli][5]\n",
    "\n",
    "```python\n",
    "torch.bernoulli(\n",
    "     # Тензор с вероятностями случайно выбрать 0 или 1.\n",
    "     # Все значения должны быть в диапазоне 0 <= input[i] <= 1\n",
    "     input,\n",
    "     *,\n",
    "     # ПГСЧ\n",
    "     generator=None,\n",
    "     # Тензор, в который выводится результат\n",
    "     out=None\n",
    ") → Tensor\n",
    "```\n",
    "\n",
    "Значения в выходном тензоре генерируются по [распределению Бернулли][13]:\n",
    "\n",
    "$$\n",
    "\\mathrm{out}_i ∼ \\mathrm{Bernoulli}(p = \\mathrm{input}_i)\n",
    "$$\n",
    "\n",
    "В возвращаемом тензоре такой же `.shape`, как у `input`.\n",
    "\n",
    "### Что ещё понадобится?\n",
    "\n",
    "[torch.ones_like][14]:\n",
    "\n",
    "```python\n",
    "torch.ones_like(\n",
    "     input,\n",
    "     *,\n",
    "     dtype=None,\n",
    "     layout=None,\n",
    "     device=None,\n",
    "     requires_grad=False,\n",
    "     memory_format=torch.preserve_format\n",
    ") → Tensor\n",
    "```\n",
    "\n",
    "Она эквивалентна вызову:\n",
    "```python\n",
    "torch.ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)\n",
    "```\n",
    "\n",
    "[1]: https://stackoverflow.com/questions/69500995/is-there-a-way-to-overide-the-backward-operation-on-nn-module\n",
    "     \"python - Is there a way to overide the backward operation on nn.Module - Stack Overflow\"\n",
    "[2]: https://discuss.pytorch.org/t/how-to-overwrite-a-backwards-pass/65845/4\n",
    "     \"How to overwrite a backwards pass - PyTorch Forums\"\n",
    "[3]: https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear\n",
    "     \"torch.nn.modules.linear — PyTorch 2.4 documentation\"\n",
    "[4]: https://pytorch.org/docs/stable/generated/torch.diag.html\n",
    "     \"torch.diag — PyTorch 2.4 documentation\"\n",
    "[5]: https://pytorch.org/docs/stable/generated/torch.bernoulli.html\n",
    "     \"torch.bernoulli — PyTorch 2.4 documentation\"\n",
    "[6]: https://pytorch.org/docs/main/notes/extending.html\n",
    "     \"Extending PyTorch — PyTorch main documentation\"\n",
    "[7]: https://pytorch.org/docs/main/notes/extending.html#extending-torch-autograd\n",
    "     \"Extending PyTorch — PyTorch main documentation\"\n",
    "[8]: https://pytorch.org/docs/main/notes/extending.html#how-to-use\n",
    "     \"Extending PyTorch — PyTorch main documentation\"\n",
    "[9]: https://pytorch.org/tutorials/intermediate/custom_function_double_backward_tutorial.html\n",
    "     \"Double Backward with Custom Functions — PyTorch Tutorials 2.4.0+cu121 documentation\"\n",
    "[10]: https://pytorch.org/docs/main/autograd.html#module-torch.autograd.gradcheck\n",
    "     \"Automatic differentiation package - torch.autograd — PyTorch main documentation\"\n",
    "[11]: https://pytorch.org/docs/main/generated/torch.autograd.gradcheck.gradcheck.html#torch.autograd.gradcheck.gradcheck\n",
    "     \"torch.autograd.gradcheck.gradcheck — PyTorch main documentation\"\n",
    "[12]: https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_\n",
    "     \"torch.nn.init — PyTorch 2.4 documentation\"\n",
    "[13]: https://en.wikipedia.org/wiki/Bernoulli_distribution\n",
    "     \"Bernoulli distribution - Wikipedia\"\n",
    "[14]: https://pytorch.org/docs/stable/generated/torch.ones_like.html\n",
    "     \"torch.ones_like — PyTorch 2.4 documentation\"\n",
    "[15]: https://pytorch.org/docs/main/generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward\n",
    "     \"https://pytorch.org/docs/main/generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    train_dataset: datasets.MNIST\n",
    "    test_dataset: datasets.MNIST\n",
    "    train_loader: torch.utils.data.DataLoader\n",
    "    test_loader: torch.utils.data.DataLoader\n",
    "\n",
    "def make_dataset(root: str, batch_size: int) -> Dataset:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST(\n",
    "        root=root,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    test_dataset = datasets.MNIST(\n",
    "        root=root,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    return Dataset(\n",
    "        train_dataset=train_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        train_loader=torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True\n",
    "        ),\n",
    "        test_loader=torch.utils.data.DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "dataset = make_dataset(root='./data', batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAHH0lEQVR4nO3cMY+MaxiA4ZljNXQSsh2VkKwgUUhINCrRaFV+gMT/0PoJKskmChFaSg2lqBQqiq1WNHynce7qnJx555yZHeu66u/J+1Zz5ynmnU/TNM0AYDab/XHQFwBgc4gCABEFACIKAEQUAIgoABBRACCiAEC2Fv1wPp+v8h4ArNgi/1W2KQAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAbB30BeB3c/bs2aXm3r9/Pzzz4MGD4ZlHjx4Nz3B42BQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEA8iAdrdvny5aXmfvz4MTzz6dOnpc7i92VTACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIA8SAerNmlS5eWmtvf3x+eefr06VJn8fuyKQAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgHgQD/6DnZ2d4Zn79+8vddbjx4+XmoMRNgUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACBeSYX/4Ny5c8Mzx48fX+qsJ0+eLDUHI2wKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAg82mapoU+nM9XfRf45bx582Z45uTJk0udtbOzMzyzv7+/1FkcTov83NsUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBAtg76ArApzpw5Mzxz5cqV4ZkPHz4Mz8xmHrdjPWwKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgHsSDn27cuLGWc758+bKWc2AZNgUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACBeSYWfLly4sJZzHj58uJZzYBk2BQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAkPk0TdNCH87nq74L/G+uXr06PPP8+fPhmY8fPw7PXLt2bXhmNpvNvn37ttQc/GWRn3ubAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAyNZBXwBW4ebNm8MzJ06cGJ55+fLl8IyH7dhkNgUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABAP4nEoXbx4cXhmmqbhmd3d3eEZ2GQ2BQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAkPm04Ctg8/l81XeBv7W9vT088+7du+GZvb294Znz588Pz8BBWeTn3qYAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBk66AvAP/m3r17wzOnTp0annnx4sXwDBw2NgUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABAP4rHxTp8+vZZz9vb21nIObDKbAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiAfx2Hi3b99eyznPnj1byzmwyWwKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgHsRjba5fv77U3Pb29v98E+Cf2BQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEA8iMfa3LlzZ6m5I0eODM+8fft2eOb169fDM3DY2BQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYB4JZWlHDt2bHjm1q1bK7jJ39vd3R2e+f79+wpuAr8WmwIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAMh8mqZpoQ/n81XfhV/I0aNHh2devXq11FmfP38enrl79+7wzNevX4dn4FeyyM+9TQGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAMSDeAC/CQ/iATBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgW4t+OE3TKu8BwAawKQAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAkD8BH5aUBGTeu5MAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw(tensor: torch.Tensor):\n",
    "    plt.imshow(tensor, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "draw(dataset.test_dataset.data[9])\n",
    "draw(dataset.test_dataset.data[2])\n",
    "draw(dataset.test_dataset.data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomBackwardFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias) -> torch.Tensor:\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        # return input * weights\n",
    "        # return input @ weights.t()\n",
    "        return F.linear(input, weight, bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Gradient formula.\n",
    "\n",
    "        It will be given as many `Tensor` arguments as there were\n",
    "        outputs, with each of them representing gradient w.r.t. that\n",
    "        output. It is important NEVER to modify these in-place. It\n",
    "        should return as many tensors as there were inputs, with each\n",
    "        of them containing the gradient w.r.t. its corresponding\n",
    "        input. If your inputs didn’t require gradient (needs_input_grad\n",
    "        is a tuple of booleans indicating whether each input needs\n",
    "        gradient computation), or were non-Tensor objects, you can\n",
    "        return `None`. Also, if you have optional arguments to\n",
    "        \"\"\"\n",
    "        input, weights, bias = ctx.saved_tensors\n",
    "\n",
    "        diag_matrix = torch.bernoulli(torch.ones_like(weights) * 0.5)\n",
    "        diag_matrix = torch.diag(diag_matrix)\n",
    "\n",
    "        grad_output = grad_output @ diag_matrix\n",
    "\n",
    "        grad_input = weights * grad_output\n",
    "        grad_weights = input * grad_output\n",
    "\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "        else:\n",
    "            grad_bias = None\n",
    "\n",
    "        return grad_input, grad_weights, grad_bias\n",
    "\n",
    "class DigitRecognizer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DigitRecognizer, self).__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(28 * 28, 32 * 32)\n",
    "        self.fc2 = nn.Linear(32 * 32, 32 * 32)\n",
    "        self.fc3 = nn.Linear(32 * 32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class DigitRecognizerCustomBackward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DigitRecognizerCustomBackward, self).__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(28 * 28, 32 * 32)\n",
    "        self.fc2 = nn.Linear(32 * 32, 32 * 32)\n",
    "        self.fc3 = nn.Linear(32 * 32, 10)\n",
    "\n",
    "        self.custom_backward = CustomBackwardFunction.apply\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.custom_backward(x, self.fc1.weight, self.fc1.bias)\n",
    "        x = F.relu(x)\n",
    "        x = self.custom_backward(x, self.fc2.weight, self.fc2.bias)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1024) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 111\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# model1 = DigitRecognizer()\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# train(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# torch.save(model1.state_dict(), 'digit_recognizer1.pth')\u001b[39;00m\n\u001b[1;32m    109\u001b[0m model2 \u001b[38;5;241m=\u001b[39m DigitRecognizerCustomBackward()\n\u001b[0;32m--> 111\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model2\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdigit_recognizer2.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, epochs, criterion, optimizer)\u001b[0m\n\u001b[1;32m     35\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 38\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m fc1_weight \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfc1\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     41\u001b[0m fc2_weight \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfc2\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.3/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.3/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.3/lib/python3.12/site-packages/torch/autograd/function.py:289\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 34\u001b[0m, in \u001b[0;36mCustomBackwardFunction.backward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m     30\u001b[0m diag_matrix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(diag_matrix)\n\u001b[1;32m     32\u001b[0m grad_output \u001b[38;5;241m=\u001b[39m grad_output \u001b[38;5;241m@\u001b[39m diag_matrix\n\u001b[0;32m---> 34\u001b[0m grad_input \u001b[38;5;241m=\u001b[39m \u001b[43mweights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgrad_output\u001b[49m\n\u001b[1;32m     35\u001b[0m grad_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m*\u001b[39m grad_output\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# grad_input = weights @ grad_output\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# grad_weights = input @ grad_output\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# grad_input = weights @ grad_output\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# grad_weights = input @ grad_output\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1024) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "MeanStd = tuple[float, float]\n",
    "\n",
    "def dabs_mean_std(\n",
    "    tensor1: torch.Tensor,\n",
    "    tensor2: torch.Tensor\n",
    ") -> MeanStd:\n",
    "    dabs: torch.Tensor = torch.abs(tensor2 - tensor1)\n",
    "\n",
    "    mean = torch.mean(dabs)\n",
    "    std = torch.std(dabs)\n",
    "\n",
    "    return (mean.item(), std.item())\n",
    "\n",
    "\n",
    "def train(model: DigitRecognizer, loader, epochs, criterion, optimizer):\n",
    "    # Наблюдаемые величины\n",
    "    running_losses: list[float] = []\n",
    "    fc1_dabs_mean: list[float] = []\n",
    "    fc1_dabs_std: list[float] = []\n",
    "    fc2_dabs_mean: list[float] = []\n",
    "    fc2_dabs_std: list[float] = []\n",
    "    fc3_dabs_mean: list[float] = []\n",
    "    fc3_dabs_std: list[float] = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            fc1_weight = model.fc1.weight.clone()\n",
    "            fc2_weight = model.fc2.weight.clone()\n",
    "            fc3_weight = model.fc3.weight.clone()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 100 == 99:\n",
    "                (mean1, std1) = (dabs_mean_std(fc1_weight, model.fc1.weight))\n",
    "                fc1_dabs_mean.append(mean1)\n",
    "                fc1_dabs_std.append(std1)\n",
    "\n",
    "                (mean2, std2) = (dabs_mean_std(fc2_weight, model.fc2.weight))\n",
    "                fc2_dabs_mean.append(mean2)\n",
    "                fc2_dabs_std.append(std2)\n",
    "\n",
    "                (mean3, std3) = (dabs_mean_std(fc3_weight, model.fc3.weight))\n",
    "                fc3_dabs_mean.append(mean3)\n",
    "                fc3_dabs_std.append(std3)\n",
    "\n",
    "                print(f'E{epoch + 1}/{epochs} S{i + 1}/{len(loader)} Loss={running_loss / 100:.4f}')\n",
    "                running_losses.append(running_loss)\n",
    "                running_loss = 0.0\n",
    "\n",
    "    X = range(len(fc1_dabs_mean))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    def plot_mean_std(label, color, series_mean, series_std):\n",
    "        plt.plot(X, series_mean, label=label, color=color)\n",
    "\n",
    "        plt.fill_between(\n",
    "            X,\n",
    "            [m - s for m, s in zip(series_mean, series_std)],\n",
    "            [m + s for m, s in zip(series_mean, series_std)],\n",
    "            color=color,\n",
    "            alpha=0.2\n",
    "        )\n",
    "\n",
    "    plot_mean_std('FC1 Mean', 'blue', fc1_dabs_mean, fc1_dabs_std)\n",
    "    plot_mean_std('FC2 Mean', 'green', fc2_dabs_mean, fc2_dabs_std)\n",
    "    plot_mean_std('FC3 Mean', 'red', fc3_dabs_mean, fc3_dabs_std)\n",
    "\n",
    "    plt.plot(X, running_losses, label='Running loss', color='orange')\n",
    "\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Mean Values with Standard Deviation for FC Layers')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# model1 = DigitRecognizer()\n",
    "\n",
    "# train(\n",
    "#     model=model1,\n",
    "#     loader=dataset.train_loader,\n",
    "#     epochs=5,\n",
    "#     criterion=nn.CrossEntropyLoss(),\n",
    "#     optimizer=optim.Adam(\n",
    "#         model1.parameters(),\n",
    "#         lr=0.001\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# torch.save(model1.state_dict(), 'digit_recognizer1.pth')\n",
    "\n",
    "model2 = DigitRecognizerCustomBackward()\n",
    "\n",
    "train(\n",
    "    model=model2,\n",
    "    loader=dataset.train_loader,\n",
    "    epochs=5,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    optimizer=optim.Adam(\n",
    "        model2.parameters(),\n",
    "        lr=0.001\n",
    "    )\n",
    ")\n",
    "\n",
    "torch.save(model2.state_dict(), 'digit_recognizer2.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.59%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model1 = DigitRecognizer()\n",
    "model1.load_state_dict(torch.load('digit_recognizer.pth'))\n",
    "\n",
    "model1.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in dataset.test_loader:\n",
    "        outputs = model1(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy: {correct / total * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.9585, -1.5439,  0.0612, -0.2481, -1.5726,  0.3865, -0.2381, -0.9189,\n",
      "        -0.7883, -0.5468], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, tensor([0]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model1 = DigitRecognizer()\n",
    "model1.load_state_dict(torch.load('digit_recognizer.pth'))\n",
    "\n",
    "idx = 997 \n",
    "\n",
    "draw(dataset.test_dataset.data[idx])\n",
    "\n",
    "(X, y) = dataset.test_dataset[idx]\n",
    "\n",
    "outputs = model1(X)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "print(outputs[0])\n",
    "\n",
    "(y, predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
