%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Flickering Glow at 2024-11-28 19:14:23 +0700 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{egnnuagtv,
	author = {Qiao, Xueting and Lin, Wanyu and Ouyang, Mingxuan and Jiang, Ting and Zhang, Ji},
	booktitle = {2024 International Joint Conference on Neural Networks (IJCNN)},
	date-added = {2024-11-28 19:12:19 +0700},
	date-modified = {2024-11-28 19:14:21 +0700},
	doi = {10.1109/IJCNN60899.2024.10650495},
	keywords = {Monte Carlo methods;Accuracy;Computational modeling;Games;Graph neural networks;Classification algorithms;Game theory;Graph Neural Networks;Explainability;Games},
	pages = {1-8},
	title = {Explanations for Graph Neural Networks using A Game-theoretic Value},
	year = {2024}}

@article{a-fusion,
	author = {Vishnu Priyan S and Vinod Kumar R and Moorthy C and Nishok VS},
	date-added = {2024-11-28 19:07:22 +0700},
	date-modified = {2024-11-28 19:10:09 +0700},
	doi = {10.3233/XST-240027},
	journal = {J Xray Sci Technol},
	keywords = {deep neural networks, generative adversarial networks, game theory},
	number = {4},
	pages = {1011-1039},
	title = {A fusion of deep neural networks and game theory for retinal disease diagnosis with OCT images},
	volume = {32},
	year = {2024}}

@article{pnnucoopgt,
	annote = {GTAP identifies and removes less impactful
neurons based on their contribution to the network's performance,
streamlining the model's size and computational load. },
	date-added = {2024-11-28 19:05:15 +0700},
	date-modified = {2024-11-28 19:06:46 +0700},
	keywords = {pruning, game theory, cooperative game theory, deep neural networks, },
	title = {Pruning Neural Networks Using Cooperative Game Theory}}

@article{hlctdrl,
	date-added = {2024-11-28 19:04:27 +0700},
	date-modified = {2024-11-28 19:05:09 +0700},
	title = {Human-level control through deep reinforcement learning}}

@article{Varga2017,
	abstract = {Regularizing the gradient norm of the output of a neural network with respect to its inputs is a powerful technique, rediscovered several times. This paper presents evidence that gradient regularization can consistently improve classification accuracy on vision tasks, using modern deep neural networks, especially when the amount of training data is small. We introduce our regularizers as members of a broader class of Jacobian-based regularizers. We demonstrate empirically on real and synthetic data that the learning process leads to gradients controlled beyond the training points, and results in solutions that generalize well.},
	author = {D{\'a}niel Varga and Adri{\'a}n Csisz{\'a}rik and Zsolt Zombori},
	date-added = {2024-11-27 14:22:59 +0700},
	date-modified = {2024-11-27 14:23:06 +0700},
	eprint = {1712.09936},
	month = {12},
	title = {Gradient Regularization Improves Accuracy of Discriminative Models},
	url = {https://arxiv.org/pdf/1712.09936.pdf},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/pdf/1712.09936.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1712.09936}}

@article{Salimans2016,
	abstract = {We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.},
	author = {Tim Salimans and Diederik P. Kingma},
	date-added = {2024-11-27 14:19:10 +0700},
	date-modified = {2024-11-27 14:19:17 +0700},
	eprint = {1602.07868},
	month = {02},
	title = {Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks},
	url = {https://arxiv.org/pdf/1602.07868.pdf},
	year = {2016},
	bdsk-url-1 = {https://arxiv.org/pdf/1602.07868.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1602.07868}}

@article{Gehring2017,
	abstract = {The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.},
	author = {Jonas Gehring and Michael Auli and David Grangier and Denis Yarats and Yann N. Dauphin},
	date-added = {2024-11-27 14:17:40 +0700},
	date-modified = {2024-11-27 14:17:49 +0700},
	eprint = {1705.03122},
	month = {05},
	title = {Convolutional Sequence to Sequence Learning},
	url = {https://arxiv.org/pdf/1705.03122.pdf},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/pdf/1705.03122.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1705.03122}}

@article{Zhang2019aa,
	abstract = {We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, \emph{gradient clipping} and \emph{normalized gradient}, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.},
	annote = {> Even though gradient clipping is a standard practice in tasks such as language models (e.g.
Merity et al., 2018; Gehring et al., 2017; Peters et al., 2018), it lacks a firm theoretical grounding

},
	author = {Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},
	bdsk-color = {4},
	date-added = {2024-11-27 14:14:07 +0700},
	date-modified = {2024-11-27 14:15:50 +0700},
	eprint = {1905.11881},
	month = {05},
	title = {Why gradient clipping accelerates training: A theoretical justification for adaptivity},
	url = {https://arxiv.org/pdf/1905.11881.pdf},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/pdf/1905.11881.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1905.11881}}

@article{shannon-1950,
	author = {Claude E. Shannon},
	date-added = {2024-11-26 10:50:43 +0700},
	date-modified = {2024-11-26 10:52:28 +0700},
	journal = {Philosophical Magazine},
	month = {March},
	number = {314},
	series = {7},
	title = {Programming a Computer for Playing Chess},
	volume = {41},
	year = {1950},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEDcuLi9TdHVkeS9QYXBlcnMvUHJvZ3JhbW1pbmdhQ29tcHV0ZXJmb3JQbGF5aW5nQ2hlc3MucGRmTxED7GJvb2vsAwAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOgCAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABzY2FsZXMAAAUAAAABAQAAU3R1ZHkAAAAGAAAAAQEAAFBhcGVycwAAJwAAAAEBAABQcm9ncmFtbWluZ2FDb21wdXRlcmZvclBsYXlpbmdDaGVzcy5wZGYAFAAAAAEGAAAEAAAAFAAAACQAAAA0AAAARAAAAAgAAAAEAwAApUEAAAAAAAAIAAAABAMAACnaBQAAAAAACAAAAAQDAAArmCQAAAAAAAgAAAAEAwAAc8dfAAAAAAAIAAAABAMAANXVaQEAAAAAFAAAAAEGAACQAAAAoAAAALAAAADAAAAA0AAAAAgAAAAABAAAQcZo6kMq9I8YAAAAAQIAAAEAAAAAAAAADwAAAAAAAAAAAAAAAAAAAAgAAAAEAwAAAwAAAAAAAAAEAAAAAwMAAPcBAAAIAAAAAQkAAGZpbGU6Ly8vDAAAAAEBAABNYWNpbnRvc2ggSEQIAAAABAMAAACQgpbnAAAACAAAAAAEAABBxi/IBoAAACQAAAABAQAANDM4NzY1ODQtNEI2OC00RUEzLUIwMDItRTQ0QTRCOTJBRDRDGAAAAAECAACBAAAAAQAAAO8TAAABAAAAAAAAAAAAAAABAAAAAQEAAC8AAAAAAAAAAQUAAPQAAAABAgAAZmI3YzMwODU1NjVhMTQ2NmNhYWU5NDY0YmVmNDAzMWZmOWU4MGY3ZmM5N2YwNThhMjE4MDg4ZDI2NDMxNDI0MzswMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDAwMDAwMDIwO2NvbS5hcHBsZS5hcHAtc2FuZGJveC5yZWFkLXdyaXRlOzAxOzAxMDAwMDEwOzAwMDAwMDAwMDE2OWQ1ZDU7MDE7L3VzZXJzL3NjYWxlcy9zdHVkeS9wYXBlcnMvcHJvZ3JhbW1pbmdhY29tcHV0ZXJmb3JwbGF5aW5nY2hlc3MucGRmAMwAAAD+////AQAAAAAAAAAQAAAABBAAAHQAAAAAAAAABRAAAOAAAAAAAAAAEBAAAAwBAAAAAAAAQBAAAPwAAAAAAAAAAiAAANgBAAAAAAAABSAAAEgBAAAAAAAAECAAAFgBAAAAAAAAESAAAIwBAAAAAAAAEiAAAGwBAAAAAAAAEyAAAHwBAAAAAAAAICAAALgBAAAAAAAAMCAAAOQBAAAAAAAAAcAAACwBAAAAAAAAEcAAABQAAAAAAAAAEsAAADwBAAAAAAAAgPAAAOwBAAAAAAAAAAgADQAaACMAXQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAARN}}

@article{parnas1978,
	author = {David L. Parnas},
	date-added = {2024-11-26 10:47:49 +0700},
	date-modified = {2024-11-27 14:18:10 +0700},
	title = {Designing Software for Ease of Extension and Contraction},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEwuLi9TdHVkeS9QYXBlcnMvRGVzaWduaW5nIFNvZnR3YXJlIGZvciBFYXNlIG9mIEV4dGVuc2lvbiBhbmQgQ29udHJhY3Rpb24ucGRmTxEEGGJvb2sYBAAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABQDAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABzY2FsZXMAAAUAAAABAQAAU3R1ZHkAAAAGAAAAAQEAAFBhcGVycwAAPAAAAAEBAABEZXNpZ25pbmcgU29mdHdhcmUgZm9yIEVhc2Ugb2YgRXh0ZW5zaW9uIGFuZCBDb250cmFjdGlvbi5wZGYUAAAAAQYAAAQAAAAUAAAAJAAAADQAAABEAAAACAAAAAQDAAClQQAAAAAAAAgAAAAEAwAAKdoFAAAAAAAIAAAABAMAACuYJAAAAAAACAAAAAQDAABzx18AAAAAAAgAAAAEAwAACRwIAAAAAAAUAAAAAQYAAKQAAAC0AAAAxAAAANQAAADkAAAACAAAAAAEAABBxcN5KVlRERgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAADAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAJCClucAAAAIAAAAAAQAAEHGL8gGgAAAJAAAAAEBAAA0Mzg3NjU4NC00QjY4LTRFQTMtQjAwMi1FNDRBNEI5MkFENEMYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAACQEAAAECAAA4MWQ1NjBkNGE4NGI5ZWQ1NTdkMTYxMGNhYjlmMjAyNGVkYmFmYjEyYTg4ODU0YzcyNjY3NDJjYzljMDQ5ODgzOzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMTA7MDAwMDAwMDAwMDA4MWMwOTswMTsvdXNlcnMvc2NhbGVzL3N0dWR5L3BhcGVycy9kZXNpZ25pbmcgc29mdHdhcmUgZm9yIGVhc2Ugb2YgZXh0ZW5zaW9uIGFuZCBjb250cmFjdGlvbi5wZGYAAAAAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAiAAAAAAAAAAFEAAA9AAAAAAAAAAQEAAAIAEAAAAAAABAEAAAEAEAAAAAAAACIAAA7AEAAAAAAAAFIAAAXAEAAAAAAAAQIAAAbAEAAAAAAAARIAAAoAEAAAAAAAASIAAAgAEAAAAAAAATIAAAkAEAAAAAAAAgIAAAzAEAAAAAAAAwIAAA+AEAAAAAAAABwAAAQAEAAAAAAAARwAAAFAAAAAAAAAASwAAAUAEAAAAAAACA8AAAAAIAAAAAAAAACAANABoAIwByAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABI4=},
	bdsk-file-2 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEwuLi9TdHVkeS9QYXBlcnMvSUVFRSBYcGxvcmUgQ2l0YXRpb24gQmliVGVYIERvd25sb2FkIDIwMjQuMTEuMjguMTkuMTEuNDcuYmliTxEEGGJvb2sYBAAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABQDAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABzY2FsZXMAAAUAAAABAQAAU3R1ZHkAAAAGAAAAAQEAAFBhcGVycwAAPAAAAAEBAABJRUVFIFhwbG9yZSBDaXRhdGlvbiBCaWJUZVggRG93bmxvYWQgMjAyNC4xMS4yOC4xOS4xMS40Ny5iaWIUAAAAAQYAAAQAAAAUAAAAJAAAADQAAABEAAAACAAAAAQDAAClQQAAAAAAAAgAAAAEAwAAKdoFAAAAAAAIAAAABAMAACuYJAAAAAAACAAAAAQDAABzx18AAAAAAAgAAAAEAwAAqoHsAQAAAAAUAAAAAQYAAKQAAAC0AAAAxAAAANQAAADkAAAACAAAAAAEAABBxnxKwadWaRgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAADAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAJCClucAAAAIAAAAAAQAAEHGL8gGgAAAJAAAAAEBAAA0Mzg3NjU4NC00QjY4LTRFQTMtQjAwMi1FNDRBNEI5MkFENEMYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAACQEAAAECAAA0MDEzNzcwODI0NTI0NzI5ZmZhOWRmY2FmZmYyZTAxOGQwYTBkZmVlMDIwN2JiNTMzMTFiOWZkZDY0ZWRmYmFiOzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMTA7MDAwMDAwMDAwMWVjODFhYTswMTsvdXNlcnMvc2NhbGVzL3N0dWR5L3BhcGVycy9pZWVlIHhwbG9yZSBjaXRhdGlvbiBiaWJ0ZXggZG93bmxvYWQgMjAyNC4xMS4yOC4xOS4xMS40Ny5iaWIAAAAAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAiAAAAAAAAAAFEAAA9AAAAAAAAAAQEAAAIAEAAAAAAABAEAAAEAEAAAAAAAACIAAA7AEAAAAAAAAFIAAAXAEAAAAAAAAQIAAAbAEAAAAAAAARIAAAoAEAAAAAAAASIAAAgAEAAAAAAAATIAAAkAEAAAAAAAAgIAAAzAEAAAAAAAAwIAAA+AEAAAAAAAABwAAAQAEAAAAAAAARwAAAFAAAAAAAAAASwAAAUAEAAAAAAACA8AAAAAIAAAAAAAAACAANABoAIwByAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABI4=}}

@article{tplpla,
	author = {Shriram Krishnamurthi},
	date-added = {2024-11-26 09:58:55 +0700},
	date-modified = {2024-11-26 09:59:12 +0700},
	title = {Teaching Programming Languages in a Post-Linnaean Age}}

@article{saip,
	date-added = {2024-11-25 17:51:40 +0700},
	date-modified = {2024-11-25 17:52:01 +0700},
	title = {Sociotropy-Autonomy and Interpersonal Problems},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEIuLi9TdHVkeS9QYXBlcnMvU29jaW90cm9weS1BdXRvbm9teSBhbmQgSW50ZXJwZXJzb25hbCBQcm9ibGVtcy5wZGZPEQQEYm9vawQEAAAAAAQQMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMAAAUAAAABAQAAVXNlcnMAAAAGAAAAAQEAAHNjYWxlcwAABQAAAAEBAABTdHVkeQAAAAYAAAABAQAAUGFwZXJzAAAyAAAAAQEAAFNvY2lvdHJvcHktQXV0b25vbXkgYW5kIEludGVycGVyc29uYWwgUHJvYmxlbXMucGRmAAAUAAAAAQYAAAQAAAAUAAAAJAAAADQAAABEAAAACAAAAAQDAAClQQAAAAAAAAgAAAAEAwAAKdoFAAAAAAAIAAAABAMAACuYJAAAAAAACAAAAAQDAABzx18AAAAAAAgAAAAEAwAAQziuAQAAAAAUAAAAAQYAAJwAAACsAAAAvAAAAMwAAADcAAAACAAAAAAEAABBxnJCeTVgsBgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAADAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAJCClucAAAAIAAAAAAQAAEHGL8gGgAAAJAAAAAEBAAA0Mzg3NjU4NC00QjY4LTRFQTMtQjAwMi1FNDRBNEI5MkFENEMYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAA/wAAAAECAAAwMTg1MDE3NDk0N2FlOTQzYjNmZTU0MDk0MDcxOTZkNzhkNWQ3M2ZlN2VhMjFjYTUyYzYxN2IxODQ5NzBmN2ZmOzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMTA7MDAwMDAwMDAwMWFlMzg0MzswMTsvdXNlcnMvc2NhbGVzL3N0dWR5L3BhcGVycy9zb2Npb3Ryb3B5LWF1dG9ub215IGFuZCBpbnRlcnBlcnNvbmFsIHByb2JsZW1zLnBkZgAAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAgAAAAAAAAAAFEAAA7AAAAAAAAAAQEAAAGAEAAAAAAABAEAAACAEAAAAAAAACIAAA5AEAAAAAAAAFIAAAVAEAAAAAAAAQIAAAZAEAAAAAAAARIAAAmAEAAAAAAAASIAAAeAEAAAAAAAATIAAAiAEAAAAAAAAgIAAAxAEAAAAAAAAwIAAA8AEAAAAAAAABwAAAOAEAAAAAAAARwAAAFAAAAAAAAAASwAAASAEAAAAAAACA8AAA+AEAAAAAAAAACAANABoAIwBoAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABHA=}}

@article{friedman2000,
	author = {Jerome H. Friedman},
	date-added = {2024-11-25 16:28:47 +0700},
	date-modified = {2024-11-25 16:30:05 +0700},
	journal = {The Annals of Statistics},
	month = {November },
	number = {5},
	pages = {1189-1232},
	title = {Greedy Function Approximation: A Gradient Boosting Machine},
	volume = {29},
	year = {2000},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEYuLi9TdHVkeS9QYXBlcnMvR3JlZWR5X0Z1bmN0aW9uX0FwcHJveGltYXRpb25fQV9HcmFkaWVudF9Cb29zdGluZ18ucGRmTxEESGJvb2tIBAAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgDAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABzY2FsZXMAAAUAAAABAQAAU3R1ZHkAAAAGAAAAAQEAAFBhcGVycwAANgAAAAEBAABHcmVlZHlfRnVuY3Rpb25fQXBwcm94aW1hdGlvbl9BX0dyYWRpZW50X0Jvb3N0aW5nXy5wZGYAABQAAAABBgAABAAAABQAAAAkAAAANAAAAEQAAAAIAAAABAMAAKVBAAAAAAAACAAAAAQDAAAp2gUAAAAAAAgAAAAEAwAAK5gkAAAAAAAIAAAABAMAAHPHXwAAAAAACAAAAAQDAACmDhgBAAAAABQAAAABBgAAoAAAALAAAADAAAAA0AAAAOAAAAAIAAAAAAQAAEHGW7yg6jv1GAAAAAECAAABAAAAAAAAAA8AAAAAAAAAAAAAAAAAAAAIAAAABAMAAAMAAAAAAAAABAAAAAMDAAD3AQAACAAAAAEJAABmaWxlOi8vLwwAAAABAQAATWFjaW50b3NoIEhECAAAAAQDAAAAkIKW5wAAAAgAAAAABAAAQcYvyAaAAAAkAAAAAQEAADQzODc2NTg0LTRCNjgtNEVBMy1CMDAyLUU0NEE0QjkyQUQ0QxgAAAABAgAAgQAAAAEAAADvEwAAAQAAAAAAAAAAAAAAAQAAAAEBAAAvAAAAAAAAAAEFAAAaAAAAAQEAAE5TVVJMRG9jdW1lbnRJZGVudGlmaWVyS2V5AAAEAAAAAwMAAFM4AQADAQAAAQIAADJlY2Q1OTAwMzk2MDYyZDhmNTY0ODlmYmQyNTk4MzYzMzNiNDdiYTU2M2EwOTA2MDlhMGMxZjMxNjU4YzhjZDE7MDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDAwMDAwMDAyMDtjb20uYXBwbGUuYXBwLXNhbmRib3gucmVhZC13cml0ZTswMTswMTAwMDAxMDswMDAwMDAwMDAxMTgwZWE2OzAxOy91c2Vycy9zY2FsZXMvc3R1ZHkvcGFwZXJzL2dyZWVkeV9mdW5jdGlvbl9hcHByb3hpbWF0aW9uX2FfZ3JhZGllbnRfYm9vc3RpbmdfLnBkZgAA2AAAAP7///8BAAAAAAAAABEAAAAEEAAAhAAAAAAAAAAFEAAA8AAAAAAAAAAQEAAAHAEAAAAAAABAEAAADAEAAAAAAAACIAAA6AEAAAAAAAAFIAAAWAEAAAAAAAAQIAAAaAEAAAAAAAARIAAAnAEAAAAAAAASIAAAfAEAAAAAAAATIAAAjAEAAAAAAAAgIAAAyAEAAAAAAAAwIAAA9AEAAAAAAAABwAAAPAEAAAAAAAARwAAAFAAAAAAAAAASwAAATAEAAAAAAACA8AAALAIAAAAAAAD8AQCAIAIAAAAAAAAACAANABoAIwBsAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABLg=}}

@book{100pmlb,
	date-added = {2024-11-25 00:11:09 +0700},
	date-modified = {2024-11-25 00:11:40 +0700},
	title = {The Hundred-Page Machine Learning Book},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEDouLi9TdHVkeS9QYXBlcnMvVGhlIEh1bmRyZWQtUGFnZSBNYWNoaW5lIExlYXJuaW5nIEJvb2sucGRmTxEEMGJvb2swBAAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACADAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABzY2FsZXMAAAUAAAABAQAAU3R1ZHkAAAAGAAAAAQEAAFBhcGVycwAAKgAAAAEBAABUaGUgSHVuZHJlZC1QYWdlIE1hY2hpbmUgTGVhcm5pbmcgQm9vay5wZGYAABQAAAABBgAABAAAABQAAAAkAAAANAAAAEQAAAAIAAAABAMAAKVBAAAAAAAACAAAAAQDAAAp2gUAAAAAAAgAAAAEAwAAK5gkAAAAAAAIAAAABAMAAHPHXwAAAAAACAAAAAQDAABTbNoBAAAAABQAAAABBgAAlAAAAKQAAAC0AAAAxAAAANQAAAAIAAAAAAQAAEHF895CVa9sGAAAAAECAAABAAAAAAAAAA8AAAAAAAAAAAAAAAAAAAAIAAAABAMAAAMAAAAAAAAABAAAAAMDAAD3AQAACAAAAAEJAABmaWxlOi8vLwwAAAABAQAATWFjaW50b3NoIEhECAAAAAQDAAAAkIKW5wAAAAgAAAAABAAAQcYvyAaAAAAkAAAAAQEAADQzODc2NTg0LTRCNjgtNEVBMy1CMDAyLUU0NEE0QjkyQUQ0QxgAAAABAgAAgQAAAAEAAADvEwAAAQAAAAAAAAAAAAAAAQAAAAEBAAAvAAAAAAAAAAEFAAAaAAAAAQEAAE5TVVJMRG9jdW1lbnRJZGVudGlmaWVyS2V5AAAEAAAAAwMAAEkAAAD3AAAAAQIAADU3ZjAxZWI0M2YwMjgyMWRkMzgzOTZjMDljMDk5NmFlMjMzMWFlN2Q2ODkwOWNiZThlYWM0Y2E2ZjMzOTBjZjA7MDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDAwMDAwMDAyMDtjb20uYXBwbGUuYXBwLXNhbmRib3gucmVhZC13cml0ZTswMTswMTAwMDAxMDswMDAwMDAwMDAxZGE2YzUzOzAxOy91c2Vycy9zY2FsZXMvc3R1ZHkvcGFwZXJzL3RoZSBodW5kcmVkLXBhZ2UgbWFjaGluZSBsZWFybmluZyBib29rLnBkZgAA2AAAAP7///8BAAAAAAAAABEAAAAEEAAAeAAAAAAAAAAFEAAA5AAAAAAAAAAQEAAAEAEAAAAAAABAEAAAAAEAAAAAAAACIAAA3AEAAAAAAAAFIAAATAEAAAAAAAAQIAAAXAEAAAAAAAARIAAAkAEAAAAAAAASIAAAcAEAAAAAAAATIAAAgAEAAAAAAAAgIAAAvAEAAAAAAAAwIAAA6AEAAAAAAAABwAAAMAEAAAAAAAARwAAAFAAAAAAAAAASwAAAQAEAAAAAAACA8AAAIAIAAAAAAADwAQCAFAIAAAAAAAAACAANABoAIwBgAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABJQ=}}

@article{Butlin2023,
	abstract = {Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also suggests that there are no obvious technical barriers to building AI systems which satisfy these indicators.},
	author = {Patrick Butlin and Robert Long and Eric Elmoznino and Yoshua Bengio and Jonathan Birch and Axel Constant and George Deane and Stephen M. Fleming and Chris Frith and Xu Ji and Ryota Kanai and Colin Klein and Grace Lindsay and Matthias Michel and Liad Mudrik and Megan A. K. Peters and Eric Schwitzgebel and Jonathan Simon and Rufin VanRullen},
	date-added = {2024-11-24 23:44:14 +0700},
	date-modified = {2024-11-27 14:18:05 +0700},
	eprint = {2308.08708},
	keywords = {consciousness, artificial intelligense},
	month = {08},
	title = {Consciousness in Artificial Intelligence: Insights from the Science of Consciousness},
	url = {https://arxiv.org/pdf/2308.08708.pdf},
	year = {2023},
	bdsk-url-1 = {https://arxiv.org/pdf/2308.08708.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/2308.08708}}

@article{santoro16,
	author = {Adam Santoro and Sergey Bartunov and Matthew Botvinick and Daan Wierstra and Timothy Lillicrap},
	date-added = {2024-11-24 23:34:14 +0700},
	date-modified = {2024-11-24 23:35:26 +0700},
	journal = {Proceedings of the 33 rd International Conference on Machine Learning},
	title = {Meta-Learning with Memory-Augmented Neural Networks},
	volume = {48},
	year = {2016},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEcuLi9TdHVkeS9QYXBlcnMvTWV0YS1MZWFybmluZyB3aXRoIE1lbW9yeS1BdWdtZW50ZWQgTmV1cmFsIE5ldHdvcmtzLnBkZk8RBAxib29rDAQAAAAABBAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAwAABQAAAAEBAABVc2VycwAAAAYAAAABAQAAc2NhbGVzAAAFAAAAAQEAAFN0dWR5AAAABgAAAAEBAABQYXBlcnMAADcAAAABAQAATWV0YS1MZWFybmluZyB3aXRoIE1lbW9yeS1BdWdtZW50ZWQgTmV1cmFsIE5ldHdvcmtzLnBkZgAUAAAAAQYAAAQAAAAUAAAAJAAAADQAAABEAAAACAAAAAQDAAClQQAAAAAAAAgAAAAEAwAAKdoFAAAAAAAIAAAABAMAACuYJAAAAAAACAAAAAQDAABzx18AAAAAAAgAAAAEAwAANGHaAQAAAAAUAAAAAQYAAKAAAACwAAAAwAAAANAAAADgAAAACAAAAAAEAABBxnnGt5EL2xgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAADAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAJCClucAAAAIAAAAAAQAAEHGL8gGgAAAJAAAAAEBAAA0Mzg3NjU4NC00QjY4LTRFQTMtQjAwMi1FNDRBNEI5MkFENEMYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAABAEAAAECAABlOWVjOTdjYTEzNmNlMWZlMThjMTk2YTBmNTExYjk2M2ZkZjBiMmJjODUxNTlhYTVhYTAwNzgzYWQzZjk0ZmI0OzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMTA7MDAwMDAwMDAwMWRhNjEzNDswMTsvdXNlcnMvc2NhbGVzL3N0dWR5L3BhcGVycy9tZXRhLWxlYXJuaW5nIHdpdGggbWVtb3J5LWF1Z21lbnRlZCBuZXVyYWwgbmV0d29ya3MucGRmAMwAAAD+////AQAAAAAAAAAQAAAABBAAAIQAAAAAAAAABRAAAPAAAAAAAAAAEBAAABwBAAAAAAAAQBAAAAwBAAAAAAAAAiAAAOgBAAAAAAAABSAAAFgBAAAAAAAAECAAAGgBAAAAAAAAESAAAJwBAAAAAAAAEiAAAHwBAAAAAAAAEyAAAIwBAAAAAAAAICAAAMgBAAAAAAAAMCAAAPQBAAAAAAAAAcAAADwBAAAAAAAAEcAAABQAAAAAAAAAEsAAAEwBAAAAAAAAgPAAAPwBAAAAAAAAAAgADQAaACMAbQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAR9}}

@article{einstein1905,
	author = {Einstein, Albert},
	journal = {Annalen der Physik},
	number = {10},
	pages = {891--921},
	publisher = {Wiley Online Library},
	title = {Zur Elektrodynamik bewegter K{\"o}rper},
	volume = {322},
	year = {1905}}

@book{dirac1930,
	author = {Dirac, Paul A. M.},
	publisher = {Clarendon Press},
	title = {The Principles of Quantum Mechanics},
	year = {1930}}

@electronic{cegt-ppt,
	annote = {Provides very good examples of open games},
	author = {Neil Ghani and Julian Hedges and Viktor Winschel and Philipp Zahn},
	date-added = {2024-11-23 16:40:12 +0700},
	date-modified = {2024-11-23 17:00:05 +0700},
	howpublished = {Website: https://conferences.inf.ed.ac.uk/ct2019/slides/ghani.pdf},
	keywords = {compositional game theory, category theory},
	month = {July},
	organization = {Edinburgh},
	title = {Compositional Economic Game Theory},
	year = {2019},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfECQuLi9TdHVkeS9QYXBlcnMvQ0dUIFByZXNlbnRhdGlvbi5wZGZPEQQEYm9vawQEAAAAAAQQMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA9AIAAAUAAAABAQAAVXNlcnMAAAAGAAAAAQEAAHNjYWxlcwAABQAAAAEBAABTdHVkeQAAAAYAAAABAQAAUGFwZXJzAAAUAAAAAQEAAENHVCBQcmVzZW50YXRpb24ucGRmFAAAAAEGAAAEAAAAFAAAACQAAAA0AAAARAAAAAgAAAAEAwAApUEAAAAAAAAIAAAABAMAACnaBQAAAAAACAAAAAQDAAArmCQAAAAAAAgAAAAEAwAAc8dfAAAAAAAIAAAABAMAAJvJ2QEAAAAAFAAAAAEGAAB8AAAAjAAAAJwAAACsAAAAvAAAAAgAAAAABAAAQcZ47KARduMYAAAAAQIAAAEAAAAAAAAADwAAAAAAAAAAAAAAAAAAAAgAAAAEAwAAAwAAAAAAAAAEAAAAAwMAAPcBAAAIAAAAAQkAAGZpbGU6Ly8vDAAAAAEBAABNYWNpbnRvc2ggSEQIAAAABAMAAACQgpbnAAAACAAAAAAEAABBxi/IBoAAACQAAAABAQAANDM4NzY1ODQtNEI2OC00RUEzLUIwMDItRTQ0QTRCOTJBRDRDGAAAAAECAACBAAAAAQAAAO8TAAABAAAAAAAAAAAAAAABAAAAAQEAAC8AAAAAAAAAAQUAABoAAAABAQAATlNVUkxEb2N1bWVudElkZW50aWZpZXJLZXkAAAQAAAADAwAA6D0BAOEAAAABAgAANDg4NTJjMTRjMGU2YjUyMWQ2ZGJhNDYxNjM5MTlkNDNkY2U2OTcyZGRkNzk3Y2U2ZTAzNzM1ZTMzYzJjZDI5NzswMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDAwMDAwMDIwO2NvbS5hcHBsZS5hcHAtc2FuZGJveC5yZWFkLXdyaXRlOzAxOzAxMDAwMDEwOzAwMDAwMDAwMDFkOWM5OWI7MDE7L3VzZXJzL3NjYWxlcy9zdHVkeS9wYXBlcnMvY2d0IHByZXNlbnRhdGlvbi5wZGYAAAAA2AAAAP7///8BAAAAAAAAABEAAAAEEAAAYAAAAAAAAAAFEAAAzAAAAAAAAAAQEAAA+AAAAAAAAABAEAAA6AAAAAAAAAACIAAAxAEAAAAAAAAFIAAANAEAAAAAAAAQIAAARAEAAAAAAAARIAAAeAEAAAAAAAASIAAAWAEAAAAAAAATIAAAaAEAAAAAAAAgIAAApAEAAAAAAAAwIAAA0AEAAAAAAAABwAAAGAEAAAAAAAARwAAAFAAAAAAAAAASwAAAKAEAAAAAAACA8AAACAIAAAAAAADYAQCA/AEAAAAAAAAACAANABoAIwBKAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABFI=}}

@article{ctrx,
	author = {олмстед},
	date-added = {2024-11-22 23:22:56 +0700},
	date-modified = {2024-11-22 23:23:14 +0700},
	title = {контрпримеры в анализе}}

@article{cat-theory,
	date-added = {2024-11-22 11:32:50 +0700},
	date-modified = {2024-11-22 11:33:02 +0700},
	title = {Учебник по теории категорий}}

@article{c4log,
	abstract = {Categories of polymorphic lenses in computer science, and of open games in compositional game theory, have a curious structure that is reminiscent of compact closed categories, but differs in some crucial ways. Specifically they have a family of morphisms that behave like the counits of a compact closed category, but have no corresponding units; and they have a `partial' duality that behaves like transposition in a compact closed category when it is defined. We axiomatise this structure, which we refer to as a `teleological category'. We precisely define a diagrammatic language suitable for these categories, and prove a coherence theorem for them. This underpins the use of diagrammatic reasoning in compositional game theory, which has previously been used only informally.},
	annote = {- formal presentation of string diagrams
- proves a coherence theorem by which we can define an open game by its string diagram},
	author = {Jules Hedges},
	bdsk-color = {3},
	date-added = {2024-11-19 17:03:44 +0700},
	date-modified = {2024-11-19 17:05:31 +0700},
	eprint = {1704.02230},
	month = {04},
	title = {Coherence for lenses and open games},
	url = {https://arxiv.org/pdf/1704.02230.pdf},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/pdf/1704.02230.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1704.02230}}

@article{abscaloops,
	author = {Samson Abramsky},
	bdsk-color = {4289007103},
	date-added = {2024-11-19 12:20:31 +0700},
	date-modified = {2024-11-19 12:21:48 +0700},
	journal = {Lectures notes in computer science},
	pages = {1-29},
	title = {Abstract scalars, loops, and free traced and strongly compact closed categories},
	volume = {3629},
	year = {2005}}

@article{proptics,
	annote = {Polymorphic lenses},
	author = {Matthew Pickering and Jeremy Gibbons and Nicolas Wu},
	bdsk-color = {3},
	date-added = {2024-11-19 12:15:22 +0700},
	date-modified = {2024-11-19 12:16:32 +0700},
	journal = {The art, science and engineering of programming},
	number = {2},
	title = {Profunctor optics: Modular data accessors},
	volume = {1},
	year = {2017}}

@phdthesis{decprop-nn,
	author = {Maria Zaitseva},
	bdsk-color = {4291887103},
	date-added = {2024-11-18 16:07:27 +0700},
	date-modified = {2024-11-23 16:45:52 +0700},
	keywords = {compositional game theory, neural networks, open games},
	rating = {5},
	school = {ITMO},
	title = {Enhancing Properties of Neural Network Using Compositional Game Theory},
	year = {2027}}

@article{seqg-opts,
	annote = {Selection functions},
	author = {Martin Escard{\'o} and Paulo Oliva},
	bdsk-color = {2},
	date-added = {2024-11-18 11:55:22 +0700},
	date-modified = {2024-11-19 12:03:02 +0700},
	journal = {Proceedings of the Royal Society A},
	pages = {1519-1545},
	title = {Sequential games and optimal strategies},
	volume = {467},
	year = {2011}}

@book{es-gt,
	annote = {Game theory introduction},
	author = {Kevin Leyton-Brown and Yoav Shoham},
	bdsk-color = {3},
	date-added = {2024-11-18 11:54:05 +0700},
	date-modified = {2024-11-18 11:54:45 +0700},
	publisher = {Morgan and Claypool},
	title = {Essentials of game theory: a concice, multidisciplinary introduction},
	year = {2008}}

@misc{moog,
	author = {Jules Hedges},
	bdsk-color = {3},
	date-added = {2024-11-18 11:50:13 +0700},
	date-modified = {2024-11-18 11:50:53 +0700},
	howpublished = {arXiv:1711.07059},
	title = {Morphisms of open games},
	year = {2017}}

@article{hiorderdecth,
	annote = {Selection functions!!
Higher order decision theory
Algorithmic decision theory},
	author = {Jules Hedges and Paulo Oliva and Evguenia Shprits and Viktor Winschel and Philipp Zahn},
	bdsk-color = {3},
	date-added = {2024-11-18 11:46:02 +0700},
	date-modified = {2024-11-19 12:13:42 +0700},
	journal = {Lecture Notes in Artificial Intelligence},
	pages = {241-254},
	title = {Higher-order decision theory},
	volume = {10576},
	year = {2017}}

@misc{coalgebra-sem,
	annote = {Treats game dynamics (?) compositionally},
	author = {Achim Blumensath and Viktor Winschel},
	bdsk-color = {3},
	date-added = {2024-11-18 11:41:15 +0700},
	date-modified = {2024-11-18 11:44:55 +0700},
	howpublished = {arXiv:1712.08381},
	title = {A compositional coalgebraic semantics of strategic games},
	year = {2013}}

@article{semapeqr,
	annote = {Discusses games as processes},
	author = {Dusko Pavlovic},
	bdsk-color = {3},
	date-added = {2024-11-18 11:36:47 +0700},
	date-modified = {2024-11-18 11:40:46 +0700},
	journal = {Lectures notes in computer science},
	pages = {317-334},
	title = {A semantical approach to equilibria and rationality},
	volume = {5728},
	year = {2008}}

@phdthesis{aois,
	annote = {Discusses categorical open systems},
	author = {Brendan Fong},
	bdsk-color = {3},
	date-added = {2024-11-18 11:30:35 +0700},
	date-modified = {2024-11-18 11:31:38 +0700},
	school = {University of Oxford},
	title = {The algebra of open and interconnected systems},
	year = {2016}}

@article{behav-open-systems,
	annote = {Discusses open systems},
	author = {Jan Willems},
	bdsk-color = {3},
	date-added = {2024-11-18 11:28:39 +0700},
	date-modified = {2024-11-18 11:30:16 +0700},
	journal = {IEEE Control Systems},
	number = {6},
	pages = {46-99},
	title = {The behavioural approach to open and interconnected systems},
	volume = {27},
	year = {2007}}

@article{backpropagation,
	author = {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
	date-added = {2024-11-17 21:00:48 +0700},
	date-modified = {2024-11-23 16:59:13 +0700},
	journal = {Nature},
	keywords = {neural networks, perceptron-convergence procedure, gradient descent},
	month = {October},
	number = {6088},
	pages = {533-536},
	title = {Learning representations by back-propagating errors},
	volume = {323},
	year = {1986},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEcuLi9TdHVkeS9QYXBlcnMvTGVhcm5pbmcgcmVwcmVzZW50YXRpb25zIGJ5IGJhY2stcHJvcGFnYXRpbmcgZXJyb3JzLnBkZk8RBEhib29rSAQAAAAABBAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4AwAABQAAAAEBAABVc2VycwAAAAYAAAABAQAAc2NhbGVzAAAFAAAAAQEAAFN0dWR5AAAABgAAAAEBAABQYXBlcnMAADcAAAABAQAATGVhcm5pbmcgcmVwcmVzZW50YXRpb25zIGJ5IGJhY2stcHJvcGFnYXRpbmcgZXJyb3JzLnBkZgAUAAAAAQYAAAQAAAAUAAAAJAAAADQAAABEAAAACAAAAAQDAAClQQAAAAAAAAgAAAAEAwAAKdoFAAAAAAAIAAAABAMAACuYJAAAAAAACAAAAAQDAABzx18AAAAAAAgAAAAEAwAAdIDEAQAAAAAUAAAAAQYAAKAAAACwAAAAwAAAANAAAADgAAAACAAAAAAEAABBxnUXZcleQBgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAADAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAJCClucAAAAIAAAAAAQAAEHGL8gGgAAAJAAAAAEBAAA0Mzg3NjU4NC00QjY4LTRFQTMtQjAwMi1FNDRBNEI5MkFENEMYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAAGgAAAAEBAABOU1VSTERvY3VtZW50SWRlbnRpZmllcktleQAABAAAAAMDAAD8OQEABAEAAAECAABkOWMzZjEzOTJmYTg2MGNjOWRhYmRlYzA3ZGFhM2NmYjE5MmJmNTEyMGRiM2M2MWE0MTU4NWY3Mjc1Y2Q2NTA3OzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMTA7MDAwMDAwMDAwMWM0ODA3NDswMTsvdXNlcnMvc2NhbGVzL3N0dWR5L3BhcGVycy9sZWFybmluZyByZXByZXNlbnRhdGlvbnMgYnkgYmFjay1wcm9wYWdhdGluZyBlcnJvcnMucGRmANgAAAD+////AQAAAAAAAAARAAAABBAAAIQAAAAAAAAABRAAAPAAAAAAAAAAEBAAABwBAAAAAAAAQBAAAAwBAAAAAAAAAiAAAOgBAAAAAAAABSAAAFgBAAAAAAAAECAAAGgBAAAAAAAAESAAAJwBAAAAAAAAEiAAAHwBAAAAAAAAEyAAAIwBAAAAAAAAICAAAMgBAAAAAAAAMCAAAPQBAAAAAAAAAcAAADwBAAAAAAAAEcAAABQAAAAAAAAAEsAAAEwBAAAAAAAAgPAAACwCAAAAAAAA/AEAgCACAAAAAAAAAAgADQAaACMAbQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAS5}}

@article{cgt,
	abstract = {We introduce open games as a compositional foundation of economic game theory. A compositional approach potentially allows methods of game theory and theoretical computer science to be applied to large-scale economic models for which standard economic tools are not practical. An open game represents a game played relative to an arbitrary environment and to this end we introduce the concept of coutility, which is the utility generated by an open game and returned to its environment. Open games are the morphisms of a symmetric monoidal category and can therefore be composed by categorical composition into sequential move games and by monoidal products into simultaneous move games. Open games can be represented by string diagrams which provide an intuitive but formal visualisation of the information flows. We show that a variety of games can be faithfully represented as open games in the sense of having the same Nash equilibria and off-equilibrium best responses.},
	author = {Neil Ghani and Jules Hedges and Viktor Winschel and Philipp Zahn},
	date-added = {2024-11-17 20:38:10 +0700},
	date-modified = {2024-11-23 17:06:12 +0700},
	eprint = {1603.04641},
	keywords = {compositionality, open games, compositional game theory, economic game theory, string diagrams, categorical composition, nash equilibrium, coutility},
	month = {03},
	rating = {4},
	read = {1},
	title = {Compositional game theory},
	url = {https://arxiv.org/pdf/1603.04641.pdf},
	year = {2016},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEC0uLi9TdHVkeS9QYXBlcnMvQ29tcG9zaXRpb25hbCBHYW1lIFRoZW9yeS5wZGZPEQQYYm9vaxgEAAAAAAQQMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAMAAAUAAAABAQAAVXNlcnMAAAAGAAAAAQEAAHNjYWxlcwAABQAAAAEBAABTdHVkeQAAAAYAAAABAQAAUGFwZXJzAAAdAAAAAQEAAENvbXBvc2l0aW9uYWwgR2FtZSBUaGVvcnkucGRmAAAAFAAAAAEGAAAEAAAAFAAAACQAAAA0AAAARAAAAAgAAAAEAwAApUEAAAAAAAAIAAAABAMAACnaBQAAAAAACAAAAAQDAAArmCQAAAAAAAgAAAAEAwAAc8dfAAAAAAAIAAAABAMAAF7N1wEAAAAAFAAAAAEGAACIAAAAmAAAAKgAAAC4AAAAyAAAAAgAAAAABAAAQcZ1Fd87mRYYAAAAAQIAAAEAAAAAAAAADwAAAAAAAAAAAAAAAAAAAAgAAAAEAwAAAwAAAAAAAAAEAAAAAwMAAPcBAAAIAAAAAQkAAGZpbGU6Ly8vDAAAAAEBAABNYWNpbnRvc2ggSEQIAAAABAMAAACQgpbnAAAACAAAAAAEAABBxi/IBoAAACQAAAABAQAANDM4NzY1ODQtNEI2OC00RUEzLUIwMDItRTQ0QTRCOTJBRDRDGAAAAAECAACBAAAAAQAAAO8TAAABAAAAAAAAAAAAAAABAAAAAQEAAC8AAAAAAAAAAQUAABoAAAABAQAATlNVUkxEb2N1bWVudElkZW50aWZpZXJLZXkAAAQAAAADAwAAjTkBAOoAAAABAgAAYTVkMzVkOWU3ZmRmY2YzM2E5NjZmODAwZjE2ODY2MWFlYmIyMzVjMWM1ZGRhNzdjMWFmYjk3ZjAzMzEzNjE4ZjswMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDAwMDAwMDIwO2NvbS5hcHBsZS5hcHAtc2FuZGJveC5yZWFkLXdyaXRlOzAxOzAxMDAwMDEwOzAwMDAwMDAwMDFkN2NkNWU7MDE7L3VzZXJzL3NjYWxlcy9zdHVkeS9wYXBlcnMvY29tcG9zaXRpb25hbCBnYW1lIHRoZW9yeS5wZGYAAADYAAAA/v///wEAAAAAAAAAEQAAAAQQAABsAAAAAAAAAAUQAADYAAAAAAAAABAQAAAEAQAAAAAAAEAQAAD0AAAAAAAAAAIgAADQAQAAAAAAAAUgAABAAQAAAAAAABAgAABQAQAAAAAAABEgAACEAQAAAAAAABIgAABkAQAAAAAAABMgAAB0AQAAAAAAACAgAACwAQAAAAAAADAgAADcAQAAAAAAAAHAAAAkAQAAAAAAABHAAAAUAAAAAAAAABLAAAA0AQAAAAAAAIDwAAAUAgAAAAAAAOQBAIAIAgAAAAAAAAAIAA0AGgAjAFMAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAAEbw==},
	bdsk-url-1 = {https://arxiv.org/pdf/1603.04641.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1603.04641}}

@phdthesis{towards-cgt,
	abstract = {We introduce a new foundation for game theory based on so-called open games. Unlike existing approaches open games are fully compositional: games are built using algebraic operations from standard components, such as players and outcome functions, with no fundamental distinction being made between the parts and the whole. Open games are intended to be applied at large scales where classical game theory becomes impractical to use, and this thesis therefore covers part of the theoretical foundation of a powerful new tool for economics and other subjects using game theory.
Formally we define a symmetric monoidal category whose morphisms are open games, which can therefore be combined either sequentially using categorical composition, or simultaneously using the monoidal product. Using this structure we can also graphically represent open games using string diagrams. We prove that the new definitions give the same results (both equilibria and off-equilibrium best responses) as classical game theory in several important special cases: normal form games with pure and mixed strategy Nash equilibria, and perfect information games with subgame perfect equilibria.
This thesis also includes work on higher order game theory, a related but simpler approach to game theory that uses higher order functions to model players. This has been extensively developed by Martin Escard ́o and Paulo Oliva for games of perfect information, and we extend it to normal form games. We show that this approach can be used to elegantly model coordination and differentiation goals of players. We also argue that a modification of the solution concept used by Escard ́o and Oliva is more appropriate for such applications.},
	annote = {Has CGT proofs},
	author = {Julian Hedges},
	bdsk-color = {3},
	date-added = {2024-11-17 20:27:13 +0700},
	date-modified = {2024-11-18 11:49:52 +0700},
	keywords = {compositional game theory, open games, higher order game theory, games of perfect infromation, normal form games},
	school = {Queen Mary University of London},
	title = {Towards compositional game theory},
	year = {2016},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEDUuLi9TdHVkeS9QYXBlcnMvVG93YXJkcyBDb21wb3NpdGlvbmFsIEdhbWUgVGhlb3J5LnBkZk8RA+xib29r7AMAAAAABBAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADoAgAABQAAAAEBAABVc2VycwAAAAYAAAABAQAAc2NhbGVzAAAFAAAAAQEAAFN0dWR5AAAABgAAAAEBAABQYXBlcnMAACUAAAABAQAAVG93YXJkcyBDb21wb3NpdGlvbmFsIEdhbWUgVGhlb3J5LnBkZgAAABQAAAABBgAABAAAABQAAAAkAAAANAAAAEQAAAAIAAAABAMAAKVBAAAAAAAACAAAAAQDAAAp2gUAAAAAAAgAAAAEAwAAK5gkAAAAAAAIAAAABAMAAHPHXwAAAAAACAAAAAQDAAC0Xt8BAAAAABQAAAABBgAAkAAAAKAAAACwAAAAwAAAANAAAAAIAAAAAAQAAEHGdRNBlGEGGAAAAAECAAABAAAAAAAAAA8AAAAAAAAAAAAAAAAAAAAIAAAABAMAAAMAAAAAAAAABAAAAAMDAAD3AQAACAAAAAEJAABmaWxlOi8vLwwAAAABAQAATWFjaW50b3NoIEhECAAAAAQDAAAAkIKW5wAAAAgAAAAABAAAQcYvyAaAAAAkAAAAAQEAADQzODc2NTg0LTRCNjgtNEVBMy1CMDAyLUU0NEE0QjkyQUQ0QxgAAAABAgAAgQAAAAEAAADvEwAAAQAAAAAAAAAAAAAAAQAAAAEBAAAvAAAAAAAAAAEFAADyAAAAAQIAADAzYjY5YmMzMzM1MzU1MmMyYzYzZThiMjUwOGU5YzQ4NGY1NjFkYjZhNzJkMmY3ZjUxOWM0NzVkNzFhMzA0OWM7MDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDAwMDAwMDAyMDtjb20uYXBwbGUuYXBwLXNhbmRib3gucmVhZC13cml0ZTswMTswMTAwMDAxMDswMDAwMDAwMDAxZGY1ZWI0OzAxOy91c2Vycy9zY2FsZXMvc3R1ZHkvcGFwZXJzL3Rvd2FyZHMgY29tcG9zaXRpb25hbCBnYW1lIHRoZW9yeS5wZGYAAADMAAAA/v///wEAAAAAAAAAEAAAAAQQAAB0AAAAAAAAAAUQAADgAAAAAAAAABAQAAAMAQAAAAAAAEAQAAD8AAAAAAAAAAIgAADYAQAAAAAAAAUgAABIAQAAAAAAABAgAABYAQAAAAAAABEgAACMAQAAAAAAABIgAABsAQAAAAAAABMgAAB8AQAAAAAAACAgAAC4AQAAAAAAADAgAADkAQAAAAAAAAHAAAAsAQAAAAAAABHAAAAUAAAAAAAAABLAAAA8AQAAAAAAAIDwAADsAQAAAAAAAAAIAA0AGgAjAFsAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAAESw==}}

@book{automl-mcs,
	author = {Frank Hutter},
	date-added = {2024-11-14 18:44:43 +0700},
	date-modified = {2024-11-14 18:52:23 +0700},
	keywords = {machine learning, automl, automation},
	publisher = {Springer},
	title = {Automated Machine Learning: Methods, Systems, Challenges},
	year = {2018}}

@article{Fan_2024,
	abstract = {This paper investigates the stability of deep ReLU neural networks for nonparametric regression under the assumption that the noise has only a finite pth moment. We unveil how the optimal rate of convergence depends on p, the degree of smoothness and the intrinsic dimension in a class of nonparametric regression functions with hierarchical composition structure when both the adaptive Huber loss and deep ReLU neural networks are used. This optimal rate of convergence cannot be obtained by the ordinary least squares but can be achieved by the Huber loss with a properly chosen parameter that adapts to the sample size, smoothness, and moment parameters. A concentration inequality for the adaptive Huber ReLU neural network estimators with allowable optimization errors is also derived. To establish a matching lower bound within the class of neural network estimators using the Huber loss, we employ a different strategy from the traditional route: constructing a deep ReLU network estimator that has a better empirical loss than the true function and the difference between these two functions furnishes a low bound. This step is related to the Huberization bias, yet more critically to the approximability of deep ReLU networks. As a result, we also contribute some new results on the approximation theory of deep ReLU neural networks.},
	author = {Fan, Jianqing and Gu, Yihong and Zhou, Wen-Xin},
	date-added = {2024-11-14 18:38:05 +0700},
	date-modified = {2024-11-14 18:53:20 +0700},
	doi = {10.1214/24-aos2428},
	issn = {0090-5364},
	journal = {The Annals of Statistics},
	keywords = {approximablility of ReLU networks , composition of functions , heavy tails , Optimal rates , robustness , truncation, machine learning},
	month = aug,
	number = {4},
	publisher = {Institute of Mathematical Statistics},
	title = {How do noise tails impact on deep ReLU networks?},
	url = {http://dx.doi.org/10.1214/24-AOS2428},
	volume = {52},
	year = {2024},
	bdsk-url-1 = {http://dx.doi.org/10.1214/24-AOS2428}}

@article{hall2023,
	abstract = {The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), leading to remarkable advancements in text understand- ing and generation. Nevertheless, alongside these strides, LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs. This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios, which attracts increasing attention to detect and mitigate these hallucina- tions. In this survey, we aim to provide a thor- ough and in-depth overview of recent advances in the field of LLM hallucinations. We begin with an innovative taxonomy of LLM halluci- nations, then delve into the factors contributing to hallucinations. Subsequently, we present a comprehensive overview of hallucination de- tection methods and benchmarks. Additionally, representative approaches designed to mitigate hallucinations are introduced accordingly. Fi- nally, we analyze the challenges that highlight the current limitations and formulate open ques- tions, aiming to delineate pathways for future research on hallucinations in LLMs. },
	author = {Lei Huang and Weijiang Yu and Weitao Ma and Weihong Zhong and Zhangyin Feng and Haotian Wang and Qianglong Chen and Weihua Peng and Xiaocheng Feng and Bing Qin and Ting Liu},
	date-added = {2024-11-14 11:11:20 +0700},
	date-modified = {2024-11-18 11:59:37 +0700},
	keywords = {nlp, llm, hallucination, survey},
	rating = {3},
	read = {0},
	title = {Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions},
	year = {2023},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEIouLi9TdHVkeS9QYXBlcnMvWmVybyBGb3JtdWxhZS9BIFN1cnZleSBvbiBIYWxsdWNpbmF0aW9uIGluIExhcmdlIExhbmd1YWdlIE1vZGVscy0gUHJpbmNpcGxlcywgVGF4b25vbXksIENoYWxsZW5nZXMsIGFuZCBPcGVuIFF1ZXN0aW9ucy5wZGZPEQS0Ym9va7QEAAAAAAQQMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAsAMAAAUAAAABAQAAVXNlcnMAAAAGAAAAAQEAAHNjYWxlcwAABQAAAAEBAABTdHVkeQAAAAYAAAABAQAAUGFwZXJzAAANAAAAAQEAAFplcm8gRm9ybXVsYWUAAABsAAAAAQEAAEEgU3VydmV5IG9uIEhhbGx1Y2luYXRpb24gaW4gTGFyZ2UgTGFuZ3VhZ2UgTW9kZWxzLSBQcmluY2lwbGVzLCBUYXhvbm9teSwgQ2hhbGxlbmdlcywgYW5kIE9wZW4gUXVlc3Rpb25zLnBkZhgAAAABBgAABAAAABQAAAAkAAAANAAAAEQAAABcAAAACAAAAAQDAAClQQAAAAAAAAgAAAAEAwAAKdoFAAAAAAAIAAAABAMAACuYJAAAAAAACAAAAAQDAABzx18AAAAAAAgAAAAEAwAAUoC/AQAAAAAIAAAABAMAAIrOdAAAAAAAGAAAAAEGAADwAAAAAAEAABABAAAgAQAAMAEAAEABAAAIAAAAAAQAAEHGRi3KsoGiGAAAAAECAAABAAAAAAAAAA8AAAAAAAAAAAAAAAAAAAAIAAAABAMAAAQAAAAAAAAABAAAAAMDAAD3AQAACAAAAAEJAABmaWxlOi8vLwwAAAABAQAATWFjaW50b3NoIEhECAAAAAQDAAAAkIKW5wAAAAgAAAAABAAAQcYvyAaAAAAkAAAAAQEAADQzODc2NTg0LTRCNjgtNEVBMy1CMDAyLUU0NEE0QjkyQUQ0QxgAAAABAgAAgQAAAAEAAADvEwAAAQAAAAAAAAAAAAAAAQAAAAEBAAAvAAAAAAAAAAEFAABHAQAAAQIAADJhNTAyY2JiYmFkZDJkOTc4Y2E2ZjlmNzZhZGMxM2JiYzY0OTQwMDEwNTAzZjQ4YzRmODdhZmY3YTczOWUyNDA7MDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDAwMDAwMDAyMDtjb20uYXBwbGUuYXBwLXNhbmRib3gucmVhZC13cml0ZTswMTswMTAwMDAxMDswMDAwMDAwMDAwNzRjZThhOzAxOy91c2Vycy9zY2FsZXMvc3R1ZHkvcGFwZXJzL3plcm8gZm9ybXVsYWUvYSBzdXJ2ZXkgb24gaGFsbHVjaW5hdGlvbiBpbiBsYXJnZSBsYW5ndWFnZSBtb2RlbHMtIHByaW5jaXBsZXMsIHRheG9ub215LCBjaGFsbGVuZ2VzLCBhbmQgb3BlbiBxdWVzdGlvbnMucGRmAADMAAAA/v///wEAAAAAAAAAEAAAAAQQAADQAAAAAAAAAAUQAABQAQAAAAAAABAQAACAAQAAAAAAAEAQAABwAQAAAAAAAAIgAABMAgAAAAAAAAUgAAC8AQAAAAAAABAgAADMAQAAAAAAABEgAAAAAgAAAAAAABIgAADgAQAAAAAAABMgAADwAQAAAAAAACAgAAAsAgAAAAAAADAgAABYAgAAAAAAAAHAAACgAQAAAAAAABHAAAAUAAAAAAAAABLAAACwAQAAAAAAAIDwAABgAgAAAAAAAAAIAA0AGgAjALAAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAAFaA==}}
