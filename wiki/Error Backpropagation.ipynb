{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Backpropagation\n",
    "\n",
    "*Error backpropagation* (or simply backpropagation) is a method for\n",
    "computing the **gradient** of the loss function with respect to the\n",
    "**weights** of the network ***citation needed***. This gradient is\n",
    "then used to update the weights in a direction that\n",
    "**reduces the loss**, typically using an **optimization algorithm**\n",
    "like stochastic gradient descent.\n",
    "\n",
    "## Passes\n",
    "\n",
    "### Forward pass\n",
    "\n",
    "During the forward pass, the *input* data is passed through the network\n",
    "**layer by layer**, and the *output* of each layer is computed using\n",
    "the current **weight** and **biases**.\n",
    "\n",
    "The final output of the network is compared to the **true labels** to\n",
    "compute the **loss** using a *loss function*.\n",
    "\n",
    "\n",
    "Consider a simple neural network with one hidden layer. Let be:\n",
    "- $x$ — input\n",
    "- $y$ — true label\n",
    "- $\\hat{y}$ — predicted output\n",
    "- $W_1$, $W_2$ — weights of layers 1 and 2\n",
    "- $b_1$, $b_2$ — biases of layers 1 and 2\n",
    "- $z_1$, $z_2$ — pre-activation outputs of layers 1 and 2\n",
    "- $a_1$, $a_2$ — activation outputs of layers 1 and 2\n",
    "- $L$ — loss\n",
    "- $\\sigma$ — activation function (sigmoid, ReLU, etc.)\n",
    "- $\\mathrm{Loss}$ — loss function (like MSE for regression tasks,\n",
    "  CE for classification…)\n",
    "\n",
    "The forward pass can be summarized as:\n",
    "\n",
    "$$\n",
    "z_1 = W_1x + b_1\n",
    "\\\\\n",
    "a_1 = \\sigma(z_1)\n",
    "\\\\\n",
    "z_2 = W_2 a_1 + b_2\n",
    "\\\\\n",
    "\\hat{y} = \\sigma(z_2)\n",
    "\\\\\n",
    "L = \\mathrm{Loss}(\\hat{y}, y)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass\n",
    "\n",
    "Starting from the *output* layer, the **gradient of the loss** with\n",
    "respect to the output is computed. It is then **propagated backward**\n",
    "through the network, layer by layer, using the **chain rule**.\n",
    "\n",
    "For each layer, the gradient of the loss with respect to the\n",
    "**weights** and **biases** is computed.\n",
    "\n",
    "The computed gradients are used to update the weights of the network.\n",
    "This is typically done using an **optimization algorithm** like SGD,\n",
    "Adam, or [RMSprop][1].\n",
    "\n",
    "Let be:\n",
    "\n",
    "- $x$ — input\n",
    "- $y$ — true label\n",
    "- $\\hat{y}$ — predicted output\n",
    "- $W_1$, $W_2$ — weights of layers 1 and 2\n",
    "- $b_1$, $b_2$ — biases of layers 1 and 2\n",
    "- $z_1$, $z_2$ — pre-activation outputs of layers 1 and 2\n",
    "- $a_1$, $a_2$ — activation outputs of layers 1 and 2\n",
    "\n",
    "We can express backward pass like this:\n",
    "\n",
    "\n",
    "$$\n",
    "\\newcommand{\\same}{\\color{blue}{}}\n",
    "\\newcommand{\\tame}{\\color{orange}{}}\n",
    "\\renewcommand{\\same}{\\color{blue}{}}\n",
    "\\renewcommand{\\tame}{\\color{orange}{}}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\tame L}{\\partial \\same{W_2}} = \\frac{\\partial \\tame L}{\\partial \\hat{y}}\n",
    "  \\cdot \\frac{\\partial \\hat{y}}{\\partial z_2}\n",
    "  \\cdot \\frac{\\partial z_2}{\\partial \\same{W_2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\tame L}{\\partial \\same{b_2}} = \\frac{\\partial \\tame L}{\\partial \\hat{y}}\n",
    "  \\cdot \\frac{\\partial \\hat{y}}{\\partial z_2}\n",
    "  \\cdot \\frac{\\partial z_2}{\\partial \\same{b_2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\tame L}{\\partial \\same{W_1}} = (\n",
    "  \\frac{\\partial \\tame L}{\\partial \\hat{y}}\n",
    "  \\cdot \\frac{\\partial \\hat{y}}{\\partial z_2}\n",
    "  \\cdot W_2\n",
    ")\n",
    "\\cdot \\frac{\\partial a_1}{\\partial z_1}\n",
    "\\cdot \\frac{\\partial z_1}{\\partial \\same{W_1}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\tame L}{\\partial \\same{b_1}} = (\n",
    "  \\frac{\\partial \\tame L}{\\partial \\hat{y}}\n",
    "  \\cdot \\frac{\\partial \\hat{y}}{\\partial z_2}\n",
    "  \\cdot W_2\n",
    ")\n",
    "\\cdot \\frac{\\partial a_1}{\\partial z_1}\n",
    "\\cdot \\frac{\\partial z_1}{\\partial \\same{b_1}}\n",
    "$$\n",
    "\n",
    "These gradients are then used to update the weights and biases:\n",
    "\n",
    "$$\n",
    "W_2 \\leftarrow W_2 - \\eta \\frac{\\partial L}{\\partial W_2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_2 \\leftarrow b_2 - \\eta \\frac{\\partial L}{\\partial b_2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_1 \\leftarrow W_1 - \\eta \\frac{\\partial L}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_1 \\leftarrow b_1 - \\eta \\frac{\\partial L}{\\partial b_1}\n",
    "$$\n",
    "\n",
    "[1]: https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- explain what passes are there and what they do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
